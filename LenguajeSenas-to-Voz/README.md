# üéôÔ∏èü§ü Traductor LSC ‚Üí Voz en Tiempo Real

Sistema completo que **detecta, reconoce y vocaliza**  
diez gestos b√°sicos de la **Lengua de Se√±as Colombiana (LSC)**  
empleando √∫nicamente una c√°mara web y la **CPU**.

<p align="center">
  <img src="Figures/demo_gui.gif" width="600">
</p>

---

## üìä Resultados

<table>
  <thead>
    <tr>
      <th style="text-align:left;">Conjunto</th>
      <th style="text-align:center;">Accuracy</th>
      <th style="text-align:center;">Macro-F<sub>1</sub></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Test</strong></td>
      <td style="text-align:center;">0.981</td>
      <td style="text-align:center;">0.981</td>
    </tr>
  </tbody>
</table>

---

## üóÇÔ∏è Estructura del repositorio

```plaintext
.
‚îú‚îÄ capture_samples.py      # Paso 1 ‚Äì Grabaci√≥n autom√°tica
‚îú‚îÄ normalize_samples.py    # Paso 2 ‚Äì Interpolaci√≥n a 15 frames
‚îú‚îÄ create_keypoints.py     # Paso 3 ‚Äì Extracci√≥n de 1 662 key-points
‚îú‚îÄ prepare_dataset.py      # Paso 4 ‚Äì Split estratificado 70/15/15
‚îú‚îÄ model.py                # Paso 5 ‚Äì Red TCN + Attention
‚îú‚îÄ training_model.py       # Paso 6 ‚Äì Entrenamiento
‚îú‚îÄ confusion_matrix.py     # Paso 7 ‚Äì M√©tricas y gr√°ficas
‚îú‚îÄ plot_pr_curves.py       # Extensi√≥n: curvas PR
‚îú‚îÄ latent_tsne_umap.py     # Extensi√≥n: t-SNE / UMAP
‚îú‚îÄ main.py                 # Paso 8 ‚Äì GUI PyQt5 en tiempo real
‚îú‚îÄ text_to_speech.py       # Paso 9 ‚Äì S√≠ntesis de voz
‚îú‚îÄ data/                   # Key-points y splits serializados
‚îú‚îÄ frame_actions/          # Frames JPG por gesto
‚îî‚îÄ models/                 # Modelo *.keras* y words.json


---

## üóÇÔ∏è Estructura del repositorio

```

.
‚îú‚îÄ capture\_samples.py      # Paso 1 ‚Äì Grabaci√≥n autom√°tica

‚îú‚îÄ normalize\_samples.py    # Paso 2 ‚Äì Interpolaci√≥n a 15 frames

‚îú‚îÄ create\_keypoints.py     # Paso 3 ‚Äì Extracci√≥n de 1 662 key-points

‚îú‚îÄ prepare\_dataset.py      # Paso 4 ‚Äì Split estratificado 70/15/15

‚îú‚îÄ model.py                # Paso 5 ‚Äì Red TCN + Attention

‚îú‚îÄ training\_model.py       # Paso 6 ‚Äì Entrenamiento

‚îú‚îÄ confusion\_matrix.py     # Paso 7 ‚Äì M√©tricas y gr√°ficas

‚îú‚îÄ plot\_pr\_curves.py       # Extensi√≥n: curvas PR

‚îú‚îÄ latent\_tsne\_umap.py     # Extensi√≥n: t-SNE / UMAP

‚îú‚îÄ main.py                 # Paso 8 ‚Äì GUI PyQt5 en tiempo real

‚îú‚îÄ text\_to\_speech.py       # Paso 9 ‚Äì S√≠ntesis de voz

‚îú‚îÄ data/                   # Key-points y splits serializados

‚îú‚îÄ frame\_actions/          # Frames JPG por gesto

‚îî‚îÄ models/                 # Modelo *.keras* y words.json

````

---

## ‚ö° Instalaci√≥n r√°pida

```bash
# 1‚ÄÇCrear y activar entorno virtual
python -m venv lsc_env
# Windows ‚ûú lsc_env\Scripts\activate
source lsc_env/bin/activate

# 2‚ÄÇInstalar dependencias (CPU-only)
pip install -r requirements.txt
#   ‚Äì TensorFlow 2.16.1
#   ‚Äì mediapipe
#   ‚Äì opencv-python
#   ‚Äì PyQt5
#   ‚Äì gTTS, pygame
#   ‚Äì seaborn, umap-learn ‚Ä¶

# 3‚ÄÇDescargar modelo pre-entrenado
curl -L -o models/actions_15.keras https://‚Ä¶/actions_15.keras
````

> El sistema corre a \~25 fps en un port√°til i5; **no se requiere GPU**.

---

## ‚ñ∂Ô∏è Demo en vivo

```bash
python main.py
```

1. Se abre la webcam.
2. Realice un gesto completo; la aplicaci√≥n lo detecta autom√°ticamente.
3. Al acabar, se muestra el texto y se reproduce la voz en castellano.

---

## üî¨ Entrenamiento desde cero

```bash
# 1‚ÄÇCapturar datos (‚âà200 muestras por gesto)
python capture_samples.py --word hola

# 2‚ÄÇNormalizar e indexar key-points
python normalize_samples.py
python create_keypoints.py

# 3‚ÄÇGenerar splits y entrenar
python prepare_dataset.py
python training_model.py            # produce models/actions_15.keras
```

---

## üìà Evaluaci√≥n y gr√°ficos adicionales

```bash
python confusion_matrix.py          # matriz + reporte
python plot_pr_curves.py            # curvas Precision‚ÄìRecall
python latent_tsne_umap.py          # mapa t-SNE / UMAP
```

---

<h2>‚ú® Componentes clave del sistema</h2>

<table>
  <thead>
    <tr>
      <th>M√≥dulo</th>
      <th>Tecnolog√≠a</th>
      <th>Rol principal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>üßç‚Äç‚ôÇÔ∏è Detecci√≥n / Tracking</td>
      <td>MediaPipe</td>
      <td>1 662 landmarks (pose + face + hands)</td>
    </tr>
    <tr>
      <td>‚è±Ô∏è Normalizaci√≥n temporal</td>
      <td>OpenCV</td>
      <td>Interpolaci√≥n / muestreo a 15 frames</td>
    </tr>
    <tr>
      <td>üß† Clasificaci√≥n secuencial</td>
      <td><strong>TCN + Attention</strong></td>
      <td>RF 31 frames, 3.5 M par√°metros, 98% accuracy</td>
    </tr>
    <tr>
      <td>üñ•Ô∏è Interfaz gr√°fica</td>
      <td>PyQt5</td>
      <td>Webcam, overlay de keypoints, texto din√°mico</td>
    </tr>
    <tr>
      <td>üîä S√≠ntesis de voz</td>
      <td>gTTS + pygame</td>
      <td>Locuci√≥n en espa√±ol con baja latencia</td>
    </tr>
  </tbody>
</table>


---

## üìù Descripci√≥n breve

El proyecto integra **detecci√≥n corporal, seguimiento temporal,
normalizaci√≥n, red convolucional dilatada con atenci√≥n escalar y
s√≠ntesis de voz** para ofrecer un traductor LSC-a-Audio port√°til,
de c√≥digo abierto y totalmente funcional en CPU.

---

<h2>üßæ Palabras entrenadas en LSC</h2>

<p>A continuaci√≥n se muestran las <strong>10 palabras</strong> reconocidas por el sistema, junto a su representaci√≥n en la <strong>Lengua de Se√±as Colombiana (LSC)</strong>.</p>

<table>
  <thead>
    <tr>
      <th>Palabra</th>
      <th>Imagen en LSC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>hola</strong></td>
      <td><img src="LenguajeSenas-to-Voz/Data/Figures/Hola.png" width="180"/></td>
    </tr>
    <tr>
      <td><strong>adios</strong></td>
      <td><img src="LenguajeSenas-to-Voz/Data/Figures/Adios.png" width="180"/></td>
    </tr>
    <tr>
      <td><strong>bien</strong></td>
      <td><img src="LenguajeSenas-to-Voz/Data/Figures/Bien.png" width="180"/></td>
    </tr>
    <tr>
      <td><strong>mal</strong></td>
      <td><img src="LenguajeSenas-to-Voz/Data/Figures/Mal.png" width="180"/></td>
    </tr>
    <tr>
      <td><strong>mas_o_menos</strong></td>
      <td><img src="LenguajeSenas-to-Voz/Data/Figures/MasOMenos.png" width="180"/></td>
    </tr>
    <tr>
      <td><strong>lo_siento</strong></td>
      <td><img src="LenguajeSenas-to-Voz/Data/Figures/LoSiento.png" width="180"/></td>
    </tr>
    <tr>
      <td><strong>como_estas</strong></td>
      <td><img src="LenguajeSenas-to-Voz/Data/Figures/ComoEstas.png" width="180"/></td>
    </tr>
    <tr>
      <td><strong>feliz_dia</strong></td>
      <td><img src="LenguajeSenas-to-Voz/Data/Figures/FelizDia.png" width="180"/></td>
    </tr>
    <tr>
      <td><strong>familia</strong></td>
      <td><img src="LenguajeSenas-to-Voz/Data/Figures/Familia.png" width="180"/></td>
    </tr>
    <tr>
      <td><strong>Papa</strong></td>
      <td><img src="LenguajeSenas-to-Voz/Data/Figures/Papa.png" width="180"/></td>
    </tr>
  </tbody>
</table>

---

## üìö Cr√©ditos

**Autores:**  Juan Sebastian Rodriguez Salazar y Johan Santiago Caro Valencia / Grupo ‚Äì Visi√≥n Computacional, 2025-I

Agradecimientos al repositorio base [@ronvidev](https://github.com/ronvidev/modelo_lstm_lsp)
por la l√≥gica original de captura.

```
```
